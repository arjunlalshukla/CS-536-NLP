{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will use a pre-trained convnet to produce features for a classifier that can detect a single object type. This notebook has some code to help you get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import pandas as pd\n",
    "import requests\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path as osp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from google_images_download import google_images_download\n",
    "\n",
    "img_folder = 'downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arguments(word):\n",
    "    args = {}\n",
    "    args['keywords'] = word\n",
    "    args['limit'] = 100\n",
    "    args['format'] = 'png'\n",
    "    args['usage_rights'] = 'labeled-for-nocommercial-reuse'\n",
    "    return args\n",
    "\n",
    "response = google_images_download.googleimagesdownload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following two cells\n",
    "\n",
    "- change the positive and negative words\n",
    "- running each cell downloads 100 images into a `downloads` folder with a subfolder of your word\n",
    "- `word` can be any string or search criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive examples\n",
    "\n",
    "word = 'red'\n",
    "absolute_image_paths = response.download(build_arguments(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative examples\n",
    "\n",
    "word = 'random'\n",
    "absolute_image_paths = response.download(build_arguments(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Run the following cell\n",
    "\n",
    "* This imports needed Keras libraries\n",
    "* Then, it gets the trained VGG19 imagenet model\n",
    "* Then, it prints out the names of all the layers in that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_conv4\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_conv4\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_conv4\n",
      "block5_pool\n",
      "flatten\n",
      "fc1\n",
      "fc2\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Determine your output layer\n",
    "\n",
    "- try `predictions` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casey/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"pr...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "layer = 'predictions'\n",
    "\n",
    "model = Model(input=base_model.input, output=base_model.get_layer(layer).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_path, xs=224,ys=224):\n",
    "    x = image.load_img(img_path, target_size=(xs, ys))\n",
    "    x = image.img_to_array(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "def get_img_features(model, img):\n",
    "    img = preprocess_input(img)\n",
    "    yhat = model.predict(img)\n",
    "    return yhat\n",
    "\n",
    "def get_image_features(word):\n",
    "    files = [f for f in listdir(osp.join(img_folder, word))] # grab all of the images in the folder\n",
    "    image_vectors = []\n",
    "    for f in tqdm(files):\n",
    "        img = get_image(osp.join(img_folder, word, f)) \n",
    "        x_feats = get_img_features(model, img).flatten() # get features for each image\n",
    "        image_vectors.append(x_feats) \n",
    "    return np.array(image_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Evaluate a classifier for your `word`\n",
    "\n",
    "* Using the positive and negative output from `base_model`, train a classifier (it can be a linear classifier from scikit-learn, if you'd like, but I would recommend the Keras Dense network we built for the previous assignment). \n",
    "* You'll need to split your data into Train and Test (I would recommend using half of the data for training, half for testing; you may opt for downloading more positive and negative examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abc3e0f64d745ceada5a7188e655302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casey/anaconda3/lib/python3.6/site-packages/PIL/Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58637a98131d408097da14cafdfa27b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_images = get_image_features('red') # get positive image vectors\n",
    "neg_images = get_image_features('random') # get negative image vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data. Split to train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Try a Different `base_model` \n",
    "\n",
    "* Repeat steps 3 and 4 above, only this time use a [pre-trained model other than VGG19](https://keras.io/applications/)\n",
    "* (Answer in a markdown cell): Which model+layer works the best for this data? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
