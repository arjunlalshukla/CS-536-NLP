{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to step you through an example of the various aspects of linguistics and give an introduction as to how linguistics can be processed computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about tokenization? \n",
    "\n",
    "Tokenization (or, in many ways, **segmentation**) is the process of creating tokens. The tokens make up the things that we want to connect together syntactically. In most western languages, this is pretty easy: we just tokenize each word based on white space. This isn't so easy for other languages (like Chinese and Japanese). The tokenization process is much more difficult and involved. \n",
    "\n",
    "We're just going to call `split` on each of our sentences (and make everything lowercase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I played with some toys',\n",
    "    'I shot an elephant in my pajamas'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(lambda x: x.lower().split(), sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'played', 'with', 'some', 'toys'],\n",
       " ['i', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do about morphology?\n",
    "\n",
    "Though not very morphologically rich, this language does use a little bit of morphology with plurals (e.g., 'sails', 'arms', 'eyes') and past tense (e.g., 'opened', 'unfurled'). Do we ignore that or try to normalize the text?\n",
    "\n",
    "* Many approaches try to scrape away extra morphological stuff and just get the lexeme\n",
    "* This process is called **lemmatization** (see also **stemming**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem.snowball as stem\n",
    "\n",
    "wnl = stem.EnglishStemmer()\n",
    "lemma_sentences = list()\n",
    "for s in sentences:\n",
    "    lemma_sentences.append([wnl.stem(t) for t in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'play', 'with', 'some', 'toy'],\n",
       " ['i', 'shot', 'an', 'eleph', 'in', 'my', 'pajama']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the grammar for this language?\n",
    "\n",
    "What are some common patterns?\n",
    "    \n",
    "* It usually starts with someone's name (or pair of names), then has some kind of prepositional phrase\n",
    "\n",
    "Can we construct a CFG? We need the following:\n",
    "\n",
    "* a set of *terminals* (that would be the words themselves)\n",
    "* a set of *non-terminal* symbols\n",
    "* a set of *production rules*\n",
    "* a *start symbol* (from within the set of non-terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | Pr\n",
    "    VP -> V NP | VP PP | V PP\n",
    "    Det -> 'an' | 'my' | 'some' \n",
    "    N -> 'eleph' | 'pajama' | 'toy'\n",
    "    V -> 'shot' | 'play'\n",
    "    P -> 'in' | 'with'\n",
    "    Pr -> 'i'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start symbol is **S**. All of the non-terminal symbols are without single quotes (and can show on the left side). The symbols with single quotes are terminals (i.e., our vocabulary) and each line is a production rule. Note that a production rule can have several possible paths (NP has a lot, as you can see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'play', 'with', 'some', 'toy']\n",
      "(S (NP (Pr i)) (VP (V play) (PP (P with) (NP (Det some) (N toy)))))\n",
      "\n",
      "\n",
      "['i', 'shot', 'an', 'eleph', 'in', 'my', 'pajama']\n",
      "(S\n",
      "  (NP (Pr i))\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N eleph)))\n",
      "    (PP (P in) (NP (Det my) (N pajama)))))\n",
      "(S\n",
      "  (NP (Pr i))\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N eleph) (PP (P in) (NP (Det my) (N pajama))))))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "for s in lemma_sentences:\n",
    "    print(s)\n",
    "    for tree in parser.parse(s):\n",
    "        print(tree)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we can parse all of the sentences, so it has full **coverage**. It also shows that our grammar **overgenerates** because it produces multiple parses for many of the sentences. When a grammar produces more than one parse for a particular sentence, then that sentence is, by definition, syntactically ambiguous. \n",
    "\n",
    "Can we parse other, novel sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Pr i))\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det some) (N toy)))\n",
      "    (PP (P with) (NP (Det my) (N pajama)))))\n",
      "(S\n",
      "  (NP (Pr i))\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det some) (N toy) (PP (P with) (NP (Det my) (N pajama))))))\n"
     ]
    }
   ],
   "source": [
    "s = 'i shot some toy with my pajama'.split()\n",
    "for tree in parser.parse(s):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As long as we keep to the vocabulary and the syntax rules (i.e., the production rules), then we can parse anything we want. \n",
    "\n",
    "Question:\n",
    "\n",
    "* How epressive is this language?\n",
    "\n",
    "Answer:\n",
    "\n",
    "* Because it has at least one recursive rule (see NP), it can generate (or be used to parse) an infinite number of sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the grammar more general\n",
    "\n",
    "From the looks of it, the above grammar is pretty set on only accepting words that are in the vocabulary. But what if we come across a word we have never heard before? Could we not still use our grammar and parser?\n",
    "\n",
    "The answer to this isn't too complicated, but it's difficult for some to wrap their minds around. Above we built a grammar where the terminals were words. What if we build a grammar where the terminals are some kind of placeholder for word types, then we determine what a new word's type is?\n",
    "\n",
    "These word types are called **parts of speech (POS)**\n",
    "\n",
    "We actually already have POS in our above grammar. Any non-terminal that has terminals on the RHS can be considered a POS. If we take away the terminal vocabulary words, we are left with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | Pr\n",
    "    VP -> V NP | VP PP | V PP\n",
    "    Det -> \n",
    "    N -> \n",
    "    V -> \n",
    "    P -> \n",
    "    Pr ->\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our set of POS tags is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = ['Det', 'N', 'V', 'P', 'Pr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our new, simplified (yet more general) grammar will be (note the quotes around the POS tags now!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> 'P' NP\n",
    "    NP -> 'Det' 'N' | 'Det' 'N' PP | 'Pr'\n",
    "    VP -> 'V' NP | VP PP | 'V' PP\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to use our grammar, we need to convert any sentence we want to parse to our POS tag set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['i', 'shot', 'an', 'eleph', 'in', 'my', 'pajama']\n",
    "p = ['Pr', 'V',   'Det', 'N',    'P',  'Det', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Pr) (VP (VP V (NP Det N)) (PP P (NP Det N))))\n",
      "(S (NP Pr) (VP V (NP Det N (PP P (NP Det N)))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "\n",
    "for tree in parser.parse(p):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is find a way to determine the POS tag for each word, then we can pass it through the grammar. This is actually a very common thing in NLP called **part-of-speech tagging**. We'll consider it in more detail in a future lecture. \n",
    "\n",
    "This kind of grammar that operates on the POS tags instead of the words is called a **non-lexicalized** grammar. The kind of grammar that we were using above which had words as part of the grammar is called a **lexicalized grammar** because the vocabulary (i.e., the *lexicon*) is directly part of the grammar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
