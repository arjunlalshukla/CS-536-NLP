{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Sentiment\n",
    "\n",
    "What is sentiment analysis? Given some text, try to determine if the sentiment of the writer of that text was positive, negative,or neutral. This is fairly easy for humans to do, but we want to automate this because we want to know if something (e.g., our restaurant, movie, book, or other product) is being talked about in real-time on social media, and, more importantly, what people think about our product. Knowing if a tweet or a post is positive or negative can help us see how our product is being received and how we can improve it. \n",
    "\n",
    "Example: sentiment about election candidates in Belgium: http://www.clips.ua.ac.be/pages/pattern-examples-elections\n",
    "\n",
    " * some data: http://help.sentiment140.com/for-students\n",
    "\n",
    "**Columns:**\n",
    "\n",
    "    0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    1 - the id of the tweet (2087)\n",
    "    2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    4 - the user that tweeted (robotickilldozr)\n",
    "    5 - the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data 1600000\n",
      "length of test 498\n",
      "length of both 1600498\n"
     ]
    }
   ],
   "source": [
    "cols = ['polarity','id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "data = pd.read_csv('data/sentiment.train.csv',names=cols, encoding='ISO-8859-1')\n",
    "print('length of data {}'.format(len(data)))\n",
    "test = pd.read_csv('data/sentiment.test.csv',names=cols, encoding='ISO-8859-1')\n",
    "print('length of test {}'.format(len(test)))\n",
    "data = pd.concat([data,test])\n",
    "print('length of both {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sample(frac=0.005,random_state=200) # this is a lot of data, while we develop let's only use 10% of it\n",
    "data = data.drop(['id', 'date', 'query', 'user'], axis=1)\n",
    "data = data[data.polarity != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "data['split_tweet'] = data.tweet.map(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>split_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>888312</th>\n",
       "      <td>4</td>\n",
       "      <td>Breaky burrito at Whole Foods is a good way to...</td>\n",
       "      <td>[Breaky, burrito, at, Whole, Foods, is, a, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516573</th>\n",
       "      <td>0</td>\n",
       "      <td>i'm out! gonna check my facebook. please!!!!!!...</td>\n",
       "      <td>[i'm, out, !, gonna, check, my, facebook, ., p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity                                              tweet  \\\n",
       "888312         4  Breaky burrito at Whole Foods is a good way to...   \n",
       "516573         0  i'm out! gonna check my facebook. please!!!!!!...   \n",
       "\n",
       "                                              split_tweet  \n",
       "888312  [Breaky, burrito, at, Whole, Foods, is, a, goo...  \n",
       "516573  [i'm, out, !, gonna, check, my, facebook, ., p...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 3960, 0: 4041})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(data.polarity) # the counts should be about the same for 0 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Test using Naive Bayes and word counts as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "\n",
    "\n",
    "data['polarity'] = data.polarity.map(lambda x: 'neg' if x == 0 else 'pos')\n",
    "data['feats'] = data['split_tweet'].map(lambda x: Counter(x))\n",
    "\n",
    "dev=data.sample(frac=0.1,random_state=200)\n",
    "train = data.drop(dev.index)\n",
    "\n",
    "train_data = list(zip(train['feats'], train['polarity']))\n",
    "dev_data = list(zip(dev['feats'], dev['polarity']))\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "nltk.classify.util.accuracy(classifier, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_len = len(list(set(dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we represent words as numerical/cotinuous features?\n",
    "\n",
    "### 1.) Try a LabelEncoder (where ['A','B','C'] = [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity'] = data.polarity.map(lambda x: 0 if x == 'neg' else 1)\n",
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set([x for line in data.split_tweet for x in line]))\n",
    "ndata = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(['the'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a long time\n",
    "\n",
    "data['le'] = data.split_tweet.map(lambda x: [le.transform([i])[0] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=data.sample(frac=0.1,random_state=200)\n",
    "train=data.drop(dev.index)\n",
    "\n",
    "train.shape, dev.shape, Counter(train.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = train[train.polarity == 0]\n",
    "pos = train[train.polarity == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.shape, pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet = max([len(v) for v in data.split_tweet])\n",
    "\n",
    "max_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = list(neg['le'].map(lambda v: np.pad(np.array(v), (0, max_tweet-len(v)), 'constant')))\n",
    "pos = list(pos['le'].map(lambda v: np.pad(np.array(v), (0, max_tweet-len(v)), 'constant')))\n",
    "labels = len(neg) * [0] + len(pos) * [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = neg + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LogisticRegression(penalty='l2')\n",
    "\n",
    "logres = regr.fit(train_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logres.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = dev[dev.polarity == 0]\n",
    "pos = dev[dev.polarity == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dev = list(neg['le'].map(lambda v: np.pad(np.array(v), (0, max_tweet-len(v)), 'constant')))\n",
    "pos_dev = list(pos['le'].map(lambda v: np.pad(np.array(v), (0, max_tweet-len(v)), 'constant')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_guess = [logres.predict(v.reshape(1, -1)) for v in neg_dev]\n",
    "pos_guess = [logres.predict(v.reshape(1, -1)) for v in pos_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(neg_guess+pos_guess, len(neg_guess)*[0] + len(pos_guess) * [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What went wrong? It's about the same as a random baseline!\n",
    "\n",
    "Answer: Semantics. \n",
    "\n",
    "The classifier uses the words represented as numbers to draw a decision boundary, but the numbers that are assigned to the words are completely arbitrary. The distance between the words (thinking about the numbers as points in some n-dimensional space) has meaning. Words that have similar meaning should be grouped together, but they aren't in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Try a One-hot encoder (where ['A','B','C'] = [[1,0,0],[0,1,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['le'], inplace=True, axis=1) # we just proved the uselessness of this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dev=data.sample(frac=0.1,random_state=200)\n",
    "train=data.drop(dev.index)\n",
    "\n",
    "train.shape, dev.shape, Counter(train.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data.split_tweet.apply(lambda x: pd.Series(x)).stack().reset_index(level=1,drop=True)\n",
    "s.name = 'word'\n",
    "data = data.drop('split_tweet', axis=1).join(s)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a very long time\n",
    "data = pd.get_dummies(data=data, columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.id.isin(train.id)]\n",
    "neg = train[train.polarity == 0].groupby('id').sum()\n",
    "pos = train[train.polarity == 1].groupby('id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = len(neg) * [0] + len(pos) * [1]\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.drop(['polarity'], axis=1, inplace=True)\n",
    "pos.drop(['polarity'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_data = pd.concat((neg,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LogisticRegression(penalty='l2')\n",
    "\n",
    "logres = regr.fit(train_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = data[data.id.isin(dev.id)]\n",
    "neg_dev = dev[dev.polarity == 0].groupby('id').sum()\n",
    "pos_dev = dev[dev.polarity == 1].groupby('id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dev.drop(['polarity'], axis=1, inplace=True)\n",
    "pos_dev.drop(['polarity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = len(neg_dev) * [0] + len(pos_dev) * [1]\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.concat((neg_dev,pos_dev))\n",
    "dev_data = dev_data.as_matrix()\n",
    "\n",
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = [logres.predict(np.array(v).reshape(1, -1)) for v in dev_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(guess, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better than chance, but still not very useful. \n",
    "\n",
    "Why does the NaiveBayesClassifier do better?\n",
    "\n",
    "* Answer: Because representing the words as string symbols instead of meaningless vectors (even if they are all equidistant) loses some of the semantic information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Try word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec as w\n",
    "\n",
    "\n",
    "w2v = w.load_word2vec_format('data/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veclen = len(w2v['red'])\n",
    "\n",
    "veclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "data['split_tweet'] = data.tweet.map(lambda x: tknzr.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['w2v'] = data.split_tweet.map(lambda v: np.sum(np.array([np.array(w2v[x]) for x in v if x in w2v]).T, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/sentiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=data.sample(frac=0.1,random_state=200)\n",
    "train=data.drop(dev.index)\n",
    "\n",
    "train.shape, dev.shape, Counter(train.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.index.isin(train.index)]\n",
    "neg = train[train.polarity == 0]\n",
    "pos = train[train.polarity == 1]\n",
    "\n",
    "neg.w2v.shape, pos.w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [x for x in pos.w2v if type(x) is not np.float64]\n",
    "neg = [x for x in neg.w2v if type(x) is not np.float64]\n",
    "labels = len(neg) * [0] + len(pos) * [1]\n",
    "\n",
    "train_data =  list(neg) + list(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LogisticRegression(penalty='l2')\n",
    "\n",
    "logres = regr.fit(train_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = data[data.index.isin(dev.index)]\n",
    "\n",
    "neg = dev[dev.polarity == 0].w2v\n",
    "pos = dev[dev.polarity == 1].w2v\n",
    "\n",
    "pos = [x for x in pos if type(x) is not np.float64]\n",
    "neg = [x for x in neg if type(x) is not np.float64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_guess = [logres.predict(v.reshape(1, -1)) for v in neg]\n",
    "pos_guess = [logres.predict(v.reshape(1, -1)) for v in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(neg_guess+pos_guess, len(neg_guess)*[0] + len(pos_guess) * [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
