{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NLP\n",
    "\n",
    "- [usage](https://stanfordnlp.github.io/stanfordnlp/installation_download.html)\n",
    "- [supported languages](https://stanfordnlp.github.io/stanfordnlp/installation_download.html)\n",
    "- [python support](https://github.com/Lynten/stanford-corenlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "import json\n",
    "\n",
    "#stanfordnlp.download('en')   # This downloads the English models for the neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/casey/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-493165aa18c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This sets up a default neural pipeline in English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Biff Tannen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0;32m---> 53\u001b[0;31m                                                                                       use_gpu=self.use_gpu)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading processors!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# set up configurations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# set up trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_final_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/models/tokenize/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "doc = nlp(\"Biff Tannen\")\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    sent.print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize: ['Guangdong', 'University', 'of', 'Foreign', 'Studies', 'is', 'located', 'in', 'Guangzhou', '.']\n",
      "Part of Speech: [('Guangdong', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Foreign', 'NNP'), ('Studies', 'NNPS'), ('is', 'VBZ'), ('located', 'JJ'), ('in', 'IN'), ('Guangzhou', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('Guangdong', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('of', 'ORGANIZATION'), ('Foreign', 'ORGANIZATION'), ('Studies', 'ORGANIZATION'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Guangzhou', 'LOCATION'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (ADJP (JJ located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 7), ('compound', 2, 1), ('nsubjpass', 7, 2), ('case', 5, 3), ('compound', 5, 4), ('nmod', 2, 5), ('auxpass', 7, 6), ('case', 9, 8), ('nmod', 7, 9), ('punct', 7, 10)]\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('/home/casey/git/stanford-corenlp-full-2017-06-09', lang='en')\n",
    "\n",
    "sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "nlp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running from Server\n",
    "\n",
    "- download [core-nlp-full](https://stanfordnlp.github.io/CoreNLP/download.html)\n",
    "- from the extracted (or cloned) folder: `java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`\n",
    "\n",
    "\n",
    "#### annotators: tokenize, ssplit, pos, lemma, ner, parse, depparse, dcoref\n",
    "[annotators info](https://github.com/Lynten/stanford-corenlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [{'index': 0,\n",
       "   'tokens': [{'index': 1,\n",
       "     'word': 'Guangdong',\n",
       "     'originalText': 'Guangdong',\n",
       "     'characterOffsetBegin': 0,\n",
       "     'characterOffsetEnd': 9,\n",
       "     'pos': 'NNP',\n",
       "     'before': '',\n",
       "     'after': ' '},\n",
       "    {'index': 2,\n",
       "     'word': 'University',\n",
       "     'originalText': 'University',\n",
       "     'characterOffsetBegin': 10,\n",
       "     'characterOffsetEnd': 20,\n",
       "     'pos': 'NNP',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 3,\n",
       "     'word': 'of',\n",
       "     'originalText': 'of',\n",
       "     'characterOffsetBegin': 21,\n",
       "     'characterOffsetEnd': 23,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 4,\n",
       "     'word': 'Foreign',\n",
       "     'originalText': 'Foreign',\n",
       "     'characterOffsetBegin': 24,\n",
       "     'characterOffsetEnd': 31,\n",
       "     'pos': 'NNP',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 5,\n",
       "     'word': 'Studies',\n",
       "     'originalText': 'Studies',\n",
       "     'characterOffsetBegin': 32,\n",
       "     'characterOffsetEnd': 39,\n",
       "     'pos': 'NNPS',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 6,\n",
       "     'word': 'is',\n",
       "     'originalText': 'is',\n",
       "     'characterOffsetBegin': 40,\n",
       "     'characterOffsetEnd': 42,\n",
       "     'pos': 'VBZ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 7,\n",
       "     'word': 'located',\n",
       "     'originalText': 'located',\n",
       "     'characterOffsetBegin': 43,\n",
       "     'characterOffsetEnd': 50,\n",
       "     'pos': 'JJ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 8,\n",
       "     'word': 'in',\n",
       "     'originalText': 'in',\n",
       "     'characterOffsetBegin': 51,\n",
       "     'characterOffsetEnd': 53,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 9,\n",
       "     'word': 'Guangzhou',\n",
       "     'originalText': 'Guangzhou',\n",
       "     'characterOffsetBegin': 54,\n",
       "     'characterOffsetEnd': 63,\n",
       "     'pos': 'NNP',\n",
       "     'before': ' ',\n",
       "     'after': ''},\n",
       "    {'index': 10,\n",
       "     'word': '.',\n",
       "     'originalText': '.',\n",
       "     'characterOffsetBegin': 63,\n",
       "     'characterOffsetEnd': 64,\n",
       "     'pos': '.',\n",
       "     'before': '',\n",
       "     'after': ' '}]},\n",
       "  {'index': 1,\n",
       "   'tokens': [{'index': 1,\n",
       "     'word': 'GDUFS',\n",
       "     'originalText': 'GDUFS',\n",
       "     'characterOffsetBegin': 65,\n",
       "     'characterOffsetEnd': 70,\n",
       "     'pos': 'NNP',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 2,\n",
       "     'word': 'is',\n",
       "     'originalText': 'is',\n",
       "     'characterOffsetBegin': 71,\n",
       "     'characterOffsetEnd': 73,\n",
       "     'pos': 'VBZ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 3,\n",
       "     'word': 'active',\n",
       "     'originalText': 'active',\n",
       "     'characterOffsetBegin': 74,\n",
       "     'characterOffsetEnd': 80,\n",
       "     'pos': 'JJ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 4,\n",
       "     'word': 'in',\n",
       "     'originalText': 'in',\n",
       "     'characterOffsetBegin': 81,\n",
       "     'characterOffsetEnd': 83,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 5,\n",
       "     'word': 'a',\n",
       "     'originalText': 'a',\n",
       "     'characterOffsetBegin': 84,\n",
       "     'characterOffsetEnd': 85,\n",
       "     'pos': 'DT',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 6,\n",
       "     'word': 'full',\n",
       "     'originalText': 'full',\n",
       "     'characterOffsetBegin': 86,\n",
       "     'characterOffsetEnd': 90,\n",
       "     'pos': 'JJ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 7,\n",
       "     'word': 'range',\n",
       "     'originalText': 'range',\n",
       "     'characterOffsetBegin': 91,\n",
       "     'characterOffsetEnd': 96,\n",
       "     'pos': 'NN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 8,\n",
       "     'word': 'of',\n",
       "     'originalText': 'of',\n",
       "     'characterOffsetBegin': 97,\n",
       "     'characterOffsetEnd': 99,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 9,\n",
       "     'word': 'international',\n",
       "     'originalText': 'international',\n",
       "     'characterOffsetBegin': 100,\n",
       "     'characterOffsetEnd': 113,\n",
       "     'pos': 'JJ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 10,\n",
       "     'word': 'cooperation',\n",
       "     'originalText': 'cooperation',\n",
       "     'characterOffsetBegin': 114,\n",
       "     'characterOffsetEnd': 125,\n",
       "     'pos': 'NN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 11,\n",
       "     'word': 'and',\n",
       "     'originalText': 'and',\n",
       "     'characterOffsetBegin': 126,\n",
       "     'characterOffsetEnd': 129,\n",
       "     'pos': 'CC',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 12,\n",
       "     'word': 'exchanges',\n",
       "     'originalText': 'exchanges',\n",
       "     'characterOffsetBegin': 130,\n",
       "     'characterOffsetEnd': 139,\n",
       "     'pos': 'NNS',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 13,\n",
       "     'word': 'in',\n",
       "     'originalText': 'in',\n",
       "     'characterOffsetBegin': 140,\n",
       "     'characterOffsetEnd': 142,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 14,\n",
       "     'word': 'education',\n",
       "     'originalText': 'education',\n",
       "     'characterOffsetBegin': 143,\n",
       "     'characterOffsetEnd': 152,\n",
       "     'pos': 'NN',\n",
       "     'before': ' ',\n",
       "     'after': ''},\n",
       "    {'index': 15,\n",
       "     'word': '.',\n",
       "     'originalText': '.',\n",
       "     'characterOffsetBegin': 152,\n",
       "     'characterOffsetEnd': 153,\n",
       "     'pos': '.',\n",
       "     'before': '',\n",
       "     'after': ''}]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "text = 'Guangdong University of Foreign Studies is located in Guangzhou. ' \\\n",
    "       'GDUFS is active in a full range of international cooperation and exchanges in education. '\n",
    "\n",
    "props={'annotators': 'tokenize,ssplit,pos','pipelineLanguage':'en','outputFormat':'json'}\n",
    "\n",
    "obj = nlp.annotate(text, properties=props)\n",
    "nlp.close()\n",
    "\n",
    "json.loads(obj) # convert from string to json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
