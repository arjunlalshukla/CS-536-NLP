{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import *\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pymagnitude import *\n",
    "import time\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/questions.csv')\n",
    "data = data\n",
    "path = 'data/GoogleNews-vectors-negative300.magnitude'\n",
    "vectors = Magnitude(path)\n",
    "data.question1 = pd.Series([str(s).lower().split() for s in data.question1])\n",
    "data.question2 = pd.Series([str(s).lower().split() for s in data.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def sum_vec(words):\n",
    "    res = np.sum([vectors.query(w) for w in words if w in vectors], axis=0)\n",
    "    if type(res) is np.float64:\n",
    "        print('encountered no vecs')\n",
    "        return np.array([1e-32] * vectors.dim)\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "Finished vec1\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "encountered no vecs\n",
      "Finished vec2\n",
      "1301.4627590179443\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "vec1 = pd.Series(sum_vec(words) for words in data.question1)\n",
    "data.insert(len(data.columns), \"vec1\", vec1)\n",
    "print(\"Finished vec1\")\n",
    "vec2 = pd.Series(sum_vec(words) for words in data.question2)\n",
    "data.insert(len(data.columns), \"vec2\", vec2)\n",
    "print(\"Finished vec2\")\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data points: 404351\n",
      "# of word vectors: 3000000\n",
      "vector dimensions: 300\n",
      "random baseline: 0.5\n",
      "most common baseline: 0.6307515005527375\n"
     ]
    }
   ],
   "source": [
    "#initial data information\n",
    "print(\"# of data points:\", len(data))\n",
    "print(\"# of word vectors:\", len(vectors))\n",
    "print('vector dimensions:', vectors.dim)\n",
    "cnt = Counter(data.is_duplicate)\n",
    "print(\"random baseline:\", 1/len(cnt.keys()))\n",
    "print(\"most common baseline:\", max([cnt[k]/sum(cnt.values()) for k in cnt.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6176663344470522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    [0 if nltk.translate.bleu_score.sentence_bleu(\n",
    "    [data.question1[i]], \n",
    "    data.question2[i]) < .5 else 1\n",
    "    for i in range(len(data))],\n",
    "    data.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "sep = int(len(data)*.75)\n",
    "#train_pnts = np.column_stack((data.vec1[:sep], data.vec2[:sep]))[0]\n",
    "#train_tgts = data.values[:sep][:,5]\n",
    "#test_pnts  = np.column_stack((data.vec1[:sep], data.vec2[:sep]))[0]\n",
    "#test_tgts  = data.values[sep:][:,5]\n",
    "\n",
    "train_pnts = data.as_matrix([\"vec1\", \"vec2\"])[:sep]\n",
    "train_pnts = [[cosine(x[0], x[1])] for x in train_pnts]\n",
    "train_tgts = data.as_matrix([\"is_duplicate\"])[:sep]\n",
    "test_pnts  = data.as_matrix([\"vec1\", \"vec2\"])[sep:]\n",
    "test_pnts  = [[cosine(x[0], x[1])] for x in test_pnts]\n",
    "test_tgts  = data.as_matrix([\"is_duplicate\"])[sep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data.values:\n",
    "    if type(x[6]) is not np.ndarray or type(x[7]) is not np.ndarray:\n",
    "        print(x[:6], type(x[6]), type(x[7]))\n",
    "    if len(x[6]) != vectors.dim or len(x[7]) != vectors.dim:\n",
    "        print(x[:6], len(x[6]), len(x[7]), vectors.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "303263\n",
      "1\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "t = train_pnts\n",
    "print(type(t))\n",
    "print(type(t[0]))\n",
    "print(len(train_pnts))\n",
    "print(len(train_pnts[0]))\n",
    "print(type(train_pnts[0][0]))\n",
    "print(type(train_tgts))\n",
    "print(type(train_tgts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6458729028173472"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(train_pnts, train_tgts)\n",
    "accuracy_score(lr.predict(test_pnts), test_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
