{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in Keras\n",
    "\n",
    "* adapted from [adventuresinmachinelearning](http://adventuresinmachinelearning.com/keras-lstm-tutorial/)\n",
    "* sequence to sequence text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to get the data: `wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz` then unzip it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the path to point to the data, if needed\n",
    "data_path = \"simple-examples/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load, shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 30\n",
    "batch_size = 20\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "\n",
    "hidden_size = 500\n",
    "num_epochs = 20\n",
    "use_dropout=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 500)           5000000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 500)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 10000)         5010000   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 30, 10000)         0         \n",
      "=================================================================\n",
      "Total params: 14,014,000\n",
      "Trainable params: 14,014,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1549/1549 [==============================] - 1854s 1s/step - loss: 6.7868 - categorical_accuracy: 0.0723 - val_loss: 6.0418 - val_categorical_accuracy: 0.1366\n",
      "\n",
      "Epoch 00001: saving model to simple-examples/data/model-01.hdf5\n",
      "Epoch 2/20\n",
      "1549/1549 [==============================] - 1623s 1s/step - loss: 5.7861 - categorical_accuracy: 0.1624 - val_loss: 5.4721 - val_categorical_accuracy: 0.1866\n",
      "\n",
      "Epoch 00002: saving model to simple-examples/data/model-02.hdf5\n",
      "Epoch 3/20\n",
      "1549/1549 [==============================] - 1648s 1s/step - loss: 5.3463 - categorical_accuracy: 0.1951 - val_loss: 5.2200 - val_categorical_accuracy: 0.2089\n",
      "\n",
      "Epoch 00003: saving model to simple-examples/data/model-03.hdf5\n",
      "Epoch 4/20\n",
      "1549/1549 [==============================] - 1611s 1s/step - loss: 5.0771 - categorical_accuracy: 0.2149 - val_loss: 5.1058 - val_categorical_accuracy: 0.2180\n",
      "\n",
      "Epoch 00004: saving model to simple-examples/data/model-04.hdf5\n",
      "Epoch 5/20\n",
      "1549/1549 [==============================] - 1581s 1s/step - loss: 4.8706 - categorical_accuracy: 0.2282 - val_loss: 5.0281 - val_categorical_accuracy: 0.2256\n",
      "\n",
      "Epoch 00005: saving model to simple-examples/data/model-05.hdf5\n",
      "Epoch 6/20\n",
      "1549/1549 [==============================] - 1587s 1s/step - loss: 4.6969 - categorical_accuracy: 0.2395 - val_loss: 4.9855 - val_categorical_accuracy: 0.2307\n",
      "\n",
      "Epoch 00006: saving model to simple-examples/data/model-06.hdf5\n",
      "Epoch 7/20\n",
      "1549/1549 [==============================] - 1589s 1s/step - loss: 4.5421 - categorical_accuracy: 0.2492 - val_loss: 4.9720 - val_categorical_accuracy: 0.2347\n",
      "\n",
      "Epoch 00007: saving model to simple-examples/data/model-07.hdf5\n",
      "Epoch 8/20\n",
      "1549/1549 [==============================] - 1592s 1s/step - loss: 4.4018 - categorical_accuracy: 0.2583 - val_loss: 5.0001 - val_categorical_accuracy: 0.2352\n",
      "\n",
      "Epoch 00008: saving model to simple-examples/data/model-08.hdf5\n",
      "Epoch 9/20\n",
      "1549/1549 [==============================] - 1583s 1s/step - loss: 4.2726 - categorical_accuracy: 0.2673 - val_loss: 4.9958 - val_categorical_accuracy: 0.2382\n",
      "\n",
      "Epoch 00009: saving model to simple-examples/data/model-09.hdf5\n",
      "Epoch 10/20\n",
      "1549/1549 [==============================] - 1593s 1s/step - loss: 4.1522 - categorical_accuracy: 0.2749 - val_loss: 5.0379 - val_categorical_accuracy: 0.2378\n",
      "\n",
      "Epoch 00010: saving model to simple-examples/data/model-10.hdf5\n",
      "Epoch 11/20\n",
      "1549/1549 [==============================] - 1590s 1s/step - loss: 4.0377 - categorical_accuracy: 0.2835 - val_loss: 5.0783 - val_categorical_accuracy: 0.2376\n",
      "\n",
      "Epoch 00011: saving model to simple-examples/data/model-11.hdf5\n",
      "Epoch 12/20\n",
      "1549/1549 [==============================] - 1581s 1s/step - loss: 3.9338 - categorical_accuracy: 0.2916 - val_loss: 5.1238 - val_categorical_accuracy: 0.2386\n",
      "\n",
      "Epoch 00012: saving model to simple-examples/data/model-12.hdf5\n",
      "Epoch 13/20\n",
      "1549/1549 [==============================] - 1584s 1s/step - loss: 3.8345 - categorical_accuracy: 0.2994 - val_loss: 5.1459 - val_categorical_accuracy: 0.2385\n",
      "\n",
      "Epoch 00013: saving model to simple-examples/data/model-13.hdf5\n",
      "Epoch 14/20\n",
      "1549/1549 [==============================] - 1581s 1s/step - loss: 3.7395 - categorical_accuracy: 0.3075 - val_loss: 5.2301 - val_categorical_accuracy: 0.2360\n",
      "\n",
      "Epoch 00014: saving model to simple-examples/data/model-14.hdf5\n",
      "Epoch 15/20\n",
      "1549/1549 [==============================] - 1579s 1s/step - loss: 3.6574 - categorical_accuracy: 0.3144 - val_loss: 5.2518 - val_categorical_accuracy: 0.2373\n",
      "\n",
      "Epoch 00015: saving model to simple-examples/data/model-15.hdf5\n",
      "Epoch 16/20\n",
      "1549/1549 [==============================] - 1579s 1s/step - loss: 3.5757 - categorical_accuracy: 0.3225 - val_loss: 5.3303 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00016: saving model to simple-examples/data/model-16.hdf5\n",
      "Epoch 17/20\n",
      "1549/1549 [==============================] - 1584s 1s/step - loss: 3.5009 - categorical_accuracy: 0.3296 - val_loss: 5.3570 - val_categorical_accuracy: 0.2349\n",
      "\n",
      "Epoch 00017: saving model to simple-examples/data/model-17.hdf5\n",
      "Epoch 18/20\n",
      "1549/1549 [==============================] - 1575s 1s/step - loss: 3.4313 - categorical_accuracy: 0.3370 - val_loss: 5.4079 - val_categorical_accuracy: 0.2347\n",
      "\n",
      "Epoch 00018: saving model to simple-examples/data/model-18.hdf5\n",
      "Epoch 19/20\n",
      "1549/1549 [==============================] - 1576s 1s/step - loss: 3.3642 - categorical_accuracy: 0.3437 - val_loss: 5.4451 - val_categorical_accuracy: 0.2340\n",
      "\n",
      "Epoch 00019: saving model to simple-examples/data/model-19.hdf5\n",
      "Epoch 20/20\n",
      "1549/1549 [==============================] - 1570s 1s/step - loss: 3.3064 - categorical_accuracy: 0.3504 - val_loss: 5.5120 - val_categorical_accuracy: 0.2328\n",
      "\n",
      "Epoch 00020: saving model to simple-examples/data/model-20.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])\n",
    "    # model.fit_generator(train_data_generator.generate(), 2000, num_epochs,\n",
    "    #                     validation_data=valid_data_generator.generate(),\n",
    "    #                     validation_steps=10)\n",
    "model.save(data_path + \"final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Actual words: director of this british industrial conglomerate <eos> a form of \n",
      "Predicted words: director of this company company company <eos> the <unk> of \n",
      "Test data:\n",
      "Actual words: futures <eos> the N stock specialist firms on the big \n",
      "Predicted words: the <eos> the <unk> <unk> market firms are the <unk> \n"
     ]
    }
   ],
   "source": [
    "model = load_model(data_path + \"/model-{}.hdf5\".format(str(num_epochs)))\n",
    "dummy_iters = 40\n",
    "example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "                                                 skip_step=1)\n",
    "print(\"Training data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_training_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "print(true_print_out)\n",
    "print(pred_print_out)\n",
    "# test data set\n",
    "dummy_iters = 40\n",
    "example_test_generator = KerasBatchGenerator(test_data, num_steps, 1, vocabulary,\n",
    "                                                 skip_step=1)\n",
    "print(\"Test data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_test_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_test_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps - 1, :])\n",
    "    true_print_out += reversed_dictionary[test_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "print(true_print_out)\n",
    "print(pred_print_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
