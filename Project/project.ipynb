{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0503 17:36:27.294411 140599383648064 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import *\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pymagnitude import *\n",
    "import time\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import bert\n",
    "from bert import *\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/questions.csv')\n",
    "data = data\n",
    "path = 'data/GoogleNews-vectors-negative300.magnitude'\n",
    "vectors = Magnitude(path)\n",
    "tok = RegexpTokenizer('\\w+')\n",
    "data.question1 = pd.Series([tok.tokenize(str(s)) for s in data.question1])\n",
    "data.question2 = pd.Series([tok.tokenize(str(s)) for s in data.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions and constants\n",
    "def sum_vec(vecs):\n",
    "    res = np.sum(vecs, axis=0)\n",
    "    if type(res) is np.float64:\n",
    "        return np.array([1e-32] * vectors.dim)\n",
    "    else:\n",
    "        return res\n",
    "    \n",
    "def rm_stop(words):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in words if not w in stop_words]\n",
    "\n",
    "def train_test(col1 ,col2):\n",
    "    sep = int(len(data)*.75)\n",
    "    ret = {}\n",
    "    ret['train_pnts'] = data.as_matrix([col1, col2])[:sep]\n",
    "    ret['train_pnts'] = [[cosine(x[0], x[1])] for x in ret['train_pnts']]\n",
    "    ret['train_tgts'] = data.as_matrix([\"is_duplicate\"])[:sep]\n",
    "    ret['test_pnts']  = data.as_matrix([col1, col2])[sep:]\n",
    "    ret['test_pnts']  = [[cosine(x[0], x[1])] for x in ret['test_pnts']]\n",
    "    ret['test_tgts']  = data.as_matrix([\"is_duplicate\"])[sep:]\n",
    "    return ret\n",
    "\n",
    "def to_vec(words):\n",
    "    return [vectors.query(w) for w in words if w in vectors]\n",
    "\n",
    "i_tup_num = 0\n",
    "i_question1 = 3\n",
    "i_question2 = 4\n",
    "i_is_duplicate = 5\n",
    "i_vec1 = 6\n",
    "i_vec2 = 7\n",
    "i_stop1 = 8\n",
    "i_stop_vec1 = 9\n",
    "i_stop2 = 10\n",
    "i_stop_vec2 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "vec1 = pd.Series(to_vec(words) for words in data.question1)\n",
    "#vec1 = pd.Series(sum_vec(words) for words in data.question1)\n",
    "data.insert(len(data.columns), \"vec1\", vec1)\n",
    "print(\"Finished vec1\")\n",
    "vec2 = pd.Series(sum_vec(words) for words in data.question2)\n",
    "data.insert(len(data.columns), \"vec2\", vec2)\n",
    "print(\"Finished vec2\")\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "stop1 = pd.Series(rm_stop(words) for words in data.question1)\n",
    "data.insert(len(data.columns), \"stop1\", stop1)\n",
    "stop_vec1 = pd.Series(sum_vec(words) for words in data.stop1)\n",
    "data.insert(len(data.columns), \"stop_vec1\", stop_vec1)\n",
    "print(\"Finished stop1\")\n",
    "stop2 = pd.Series(rm_stop(words) for words in data.question2)\n",
    "data.insert(len(data.columns), \"stop2\", stop2)\n",
    "stop_vec2 = pd.Series(sum_vec(words) for words in data.stop2)\n",
    "data.insert(len(data.columns), \"stop_vec2\", stop_vec1)\n",
    "print(\"Finished stop2\")\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial data information\n",
    "print(\"# of data points:\", len(data))\n",
    "print(\"# of word vectors:\", len(vectors))\n",
    "print('vector dimensions:', vectors.dim)\n",
    "cnt = Counter(data.is_duplicate)\n",
    "print(\"random baseline:\", 1/len(cnt.keys()))\n",
    "print(\"most common baseline:\", max([cnt[k]/sum(cnt.values()) for k in cnt.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(\n",
    "    [0 if nltk.translate.bleu_score.sentence_bleu(\n",
    "    [data.question1[i]], \n",
    "    data.question2[i]) < .5 else 1\n",
    "    for i in range(len(data))],\n",
    "    data.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_test(\"vec1\", \"vec2\")\n",
    "lr = LogisticRegression().fit(x['train_pnts'], x['train_tgts'])\n",
    "print(\"normal embeddings LR accuracy:\", accuracy_score(lr.predict(x['test_pnts']), x['test_tgts']))\n",
    "print(\"normal embeddings LR coefficients:\", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_test(\"stop_vec1\", \"stop_vec2\")\n",
    "lr = LogisticRegression().fit(x['train_pnts'], x['train_tgts'])\n",
    "print(\"no stop words LR accuracy:\", accuracy_score(lr.predict(x['test_pnts']), x['test_tgts']))\n",
    "print(\"no stop words LR coefficients:\", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x in data.values:\n",
    "    if x[i_question1] == x[i_question2]:\n",
    "        print(x[i_is_duplicate], x[i_tup_num])\n",
    "        print(x[i_question1])\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = data.as_matrix([\"vec1\", \"vec2\"])\n",
    "hello = [[cosine(x[0], x[1])] for x in hello]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.values\n",
    "k = 0\n",
    "for i in range(len(data.values)):\n",
    "    if hello[i] == [0.0]:\n",
    "        print(x[i][i_is_duplicate], x[i][i_tup_num])\n",
    "        print(x[i][i_question1])\n",
    "        print(x[i][i_question2])\n",
    "        k += 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vectors.query(\"PhD\"), vectors.query(\"Ph.D.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"WW1\" in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Chick-fil-A\" in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[299692:299692+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "trees = treebank.parsed_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'dsafa'\n",
    "x.hello = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(tree):\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return child_nodes\n",
    "\n",
    "trees = [subtree for tree in treebank.parsed_sents() for subtree in tree.subtrees(filter)]\n",
    "\n",
    "production_rules = defaultdict(list)\n",
    "terminals = list()\n",
    "\n",
    "for tree in trees:\n",
    "    for prod in tree.productions():\n",
    "        lhs = str(prod.lhs()).replace('=','-').replace('|','-')\n",
    "        if prod.is_lexical():\n",
    "            terminals.append(lhs)\n",
    "        if prod.is_nonlexical():\n",
    "            rhs = ''\n",
    "            for s in prod.rhs(): \n",
    "                rhs += str(s).replace('=','-').replace('|','-') + ' '\n",
    "            production_rules[lhs].append(str(rhs.strip()))\n",
    "            \n",
    "terminals = set(terminals)\n",
    "terminals.remove(\"''\")\n",
    "\n",
    "production_counters = dict()\n",
    "\n",
    "for prod in production_rules:\n",
    "    production_counters[prod] = Counter(production_rules[prod])\n",
    "\n",
    "def to_prod_rules(lhs, c):\n",
    "    global terminals\n",
    "    denom = sum(c.values())\n",
    "    cfg_str = list()\n",
    "    for rhs in c:\n",
    "        rhs_prods = rhs.split()\n",
    "        r = ''\n",
    "        for rule in rhs_prods: \n",
    "            if rule == '' or rule == \"''\": continue\n",
    "            r += \"'\" + rule + \"' \" if rule in terminals else rule + \" \"\n",
    "        cfg_str.append(\"{} -> {} [{}]\".format(lhs, r.strip(), '{:.20f}'.format(c[rhs] / denom)))\n",
    "    return cfg_str\n",
    "\n",
    "production_lhs = list(production_counters.keys())\n",
    "production_lhs.remove('S')\n",
    "production_lhs = ['S'] + production_lhs\n",
    "\n",
    "prod_rules = list()\n",
    "\n",
    "for lhs in production_lhs:\n",
    "    prod_rules += to_prod_rules(lhs, production_counters[lhs])\n",
    "    \n",
    "prod_rules_str = '\\n'.join(prod_rules)\n",
    "\n",
    "grammar = nltk.PCFG.fromstring(prod_rules_str)\n",
    "\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['I', 'like', 'to', 'eat', 'tacos']\n",
    "\n",
    "pos_sentence = nltk.pos_tag(sentence)\n",
    "\n",
    "print(sentence)\n",
    "print(pos_sentence)\n",
    "\n",
    "\n",
    "for tree in viterbi_parser.parse([x[1] for x in pos_sentence]):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.question1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for h in dir(requests.packages.idna):\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=6o2a5ZIvRcJq\n",
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  labels = [\"Negative\", \"Positive\"]\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()\n",
    "train = train.sample(5000)\n",
    "test = test.sample(5000)\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:14.839184 140109432092480 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.288258 140109432092480 run_classifier.py:774] Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.291366 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.292389 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] / the first episode i saw of lost made me think , i thought what is this some people who crashed and get chased by a giant monster . but it ' s not like that , it ' s far more than that , because their is no monster at all and every episode that you see of lost , well it ' s getting better every time . a deserted island with an underground bunker and especially the connection between the people who crossed paths with each other before they crashed . that ' s the real secret . < br / > < br / > this series rules and i can ' t wait to know what ' s really going on [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.293364 140109432092480 run_classifier.py:464] tokens: [CLS] / the first episode i saw of lost made me think , i thought what is this some people who crashed and get chased by a giant monster . but it ' s not like that , it ' s far more than that , because their is no monster at all and every episode that you see of lost , well it ' s getting better every time . a deserted island with an underground bunker and especially the connection between the people who crossed paths with each other before they crashed . that ' s the real secret . < br / > < br / > this series rules and i can ' t wait to know what ' s really going on [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1013 1996 2034 2792 1045 2387 1997 2439 2081 2033 2228 1010 1045 2245 2054 2003 2023 2070 2111 2040 8007 1998 2131 13303 2011 1037 5016 6071 1012 2021 2009 1005 1055 2025 2066 2008 1010 2009 1005 1055 2521 2062 2084 2008 1010 2138 2037 2003 2053 6071 2012 2035 1998 2296 2792 2008 2017 2156 1997 2439 1010 2092 2009 1005 1055 2893 2488 2296 2051 1012 1037 12768 2479 2007 2019 5230 15742 1998 2926 1996 4434 2090 1996 2111 2040 4625 10425 2007 2169 2060 2077 2027 8007 1012 2008 1005 1055 1996 2613 3595 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2186 3513 1998 1045 2064 1005 1056 3524 2000 2113 2054 1005 1055 2428 2183 2006 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.294257 140109432092480 run_classifier.py:465] input_ids: 101 1013 1996 2034 2792 1045 2387 1997 2439 2081 2033 2228 1010 1045 2245 2054 2003 2023 2070 2111 2040 8007 1998 2131 13303 2011 1037 5016 6071 1012 2021 2009 1005 1055 2025 2066 2008 1010 2009 1005 1055 2521 2062 2084 2008 1010 2138 2037 2003 2053 6071 2012 2035 1998 2296 2792 2008 2017 2156 1997 2439 1010 2092 2009 1005 1055 2893 2488 2296 2051 1012 1037 12768 2479 2007 2019 5230 15742 1998 2926 1996 4434 2090 1996 2111 2040 4625 10425 2007 2169 2060 2077 2027 8007 1012 2008 1005 1055 1996 2613 3595 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2186 3513 1998 1045 2064 1005 1056 3524 2000 2113 2054 1005 1055 2428 2183 2006 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.295413 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.296195 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.297182 140109432092480 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.301258 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.302364 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] most war films made in the us during wwii were great fun to watch but suffered from severe gaps in realism because they were being produced more for propaganda value to raise the spirits at home than anything else . i am not knocking these films as many of them are still very watch ##able . however , because they so often lack realism they are prevented from being truly great films . a perfect example was the john garfield film air force - - in which a b - 17 nearly single - handed ##ly takes out half the japanese air force ! however , pride of the marines is a welcome departure - - scoring high marks for portraying a true story in a [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.303428 140109432092480 run_classifier.py:464] tokens: [CLS] most war films made in the us during wwii were great fun to watch but suffered from severe gaps in realism because they were being produced more for propaganda value to raise the spirits at home than anything else . i am not knocking these films as many of them are still very watch ##able . however , because they so often lack realism they are prevented from being truly great films . a perfect example was the john garfield film air force - - in which a b - 17 nearly single - handed ##ly takes out half the japanese air force ! however , pride of the marines is a welcome departure - - scoring high marks for portraying a true story in a [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2087 2162 3152 2081 1999 1996 2149 2076 25755 2020 2307 4569 2000 3422 2021 4265 2013 5729 16680 1999 15650 2138 2027 2020 2108 2550 2062 2005 10398 3643 2000 5333 1996 8633 2012 2188 2084 2505 2842 1012 1045 2572 2025 10591 2122 3152 2004 2116 1997 2068 2024 2145 2200 3422 3085 1012 2174 1010 2138 2027 2061 2411 3768 15650 2027 2024 8729 2013 2108 5621 2307 3152 1012 1037 3819 2742 2001 1996 2198 20170 2143 2250 2486 1011 1011 1999 2029 1037 1038 1011 2459 3053 2309 1011 4375 2135 3138 2041 2431 1996 2887 2250 2486 999 2174 1010 6620 1997 1996 9622 2003 1037 6160 6712 1011 1011 4577 2152 6017 2005 17274 1037 2995 2466 1999 1037 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.304271 140109432092480 run_classifier.py:465] input_ids: 101 2087 2162 3152 2081 1999 1996 2149 2076 25755 2020 2307 4569 2000 3422 2021 4265 2013 5729 16680 1999 15650 2138 2027 2020 2108 2550 2062 2005 10398 3643 2000 5333 1996 8633 2012 2188 2084 2505 2842 1012 1045 2572 2025 10591 2122 3152 2004 2116 1997 2068 2024 2145 2200 3422 3085 1012 2174 1010 2138 2027 2061 2411 3768 15650 2027 2024 8729 2013 2108 5621 2307 3152 1012 1037 3819 2742 2001 1996 2198 20170 2143 2250 2486 1011 1011 1999 2029 1037 1038 1011 2459 3053 2309 1011 4375 2135 3138 2041 2431 1996 2887 2250 2486 999 2174 1010 6620 1997 1996 9622 2003 1037 6160 6712 1011 1011 4577 2152 6017 2005 17274 1037 2995 2466 1999 1037 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.305176 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.306306 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.307131 140109432092480 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.308973 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.309865 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] in the opinion of several of my friends and family members , including myself , this is the finest of the entire ga ##mut of tarzan movies . johnny weiss ##mu ##ller never played the part as well in the following issues in the series . it definitely rates a \" 10 \" in my collection of films . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.310657 140109432092480 run_classifier.py:464] tokens: [CLS] in the opinion of several of my friends and family members , including myself , this is the finest of the entire ga ##mut of tarzan movies . johnny weiss ##mu ##ller never played the part as well in the following issues in the series . it definitely rates a \" 10 \" in my collection of films . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1999 1996 5448 1997 2195 1997 2026 2814 1998 2155 2372 1010 2164 2870 1010 2023 2003 1996 10418 1997 1996 2972 11721 28120 1997 24566 5691 1012 5206 17889 12274 10820 2196 2209 1996 2112 2004 2092 1999 1996 2206 3314 1999 1996 2186 1012 2009 5791 6165 1037 1000 2184 1000 1999 2026 3074 1997 3152 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.312826 140109432092480 run_classifier.py:465] input_ids: 101 1999 1996 5448 1997 2195 1997 2026 2814 1998 2155 2372 1010 2164 2870 1010 2023 2003 1996 10418 1997 1996 2972 11721 28120 1997 24566 5691 1012 5206 17889 12274 10820 2196 2209 1996 2112 2004 2092 1999 1996 2206 3314 1999 1996 2186 1012 2009 5791 6165 1037 1000 2184 1000 1999 2026 3074 1997 3152 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.314142 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.315346 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.322196 140109432092480 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.326894 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.327811 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i basically found eden ' s curve to be a very poorly constructed that made it difficult to watch . however , there is something i must say about how the director captured something about the atmosphere of the early 70 ' s in the choice of settings and clothing . the \" back to the earth \" philosophy and the interest in sexual exploration and drugs that was not dramatically decade ##nt , as portrayed in many later versions of the 70 ' s was right on , as was the \" don ' t ask don ' t tell \" pseudo - liberalism of the fraternity made up of east - coast intellectuals , except that i would have thought this was more likely [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.328704 140109432092480 run_classifier.py:464] tokens: [CLS] i basically found eden ' s curve to be a very poorly constructed that made it difficult to watch . however , there is something i must say about how the director captured something about the atmosphere of the early 70 ' s in the choice of settings and clothing . the \" back to the earth \" philosophy and the interest in sexual exploration and drugs that was not dramatically decade ##nt , as portrayed in many later versions of the 70 ' s was right on , as was the \" don ' t ask don ' t tell \" pseudo - liberalism of the fraternity made up of east - coast intellectuals , except that i would have thought this was more likely [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 10468 2179 10267 1005 1055 7774 2000 2022 1037 2200 9996 3833 2008 2081 2009 3697 2000 3422 1012 2174 1010 2045 2003 2242 1045 2442 2360 2055 2129 1996 2472 4110 2242 2055 1996 7224 1997 1996 2220 3963 1005 1055 1999 1996 3601 1997 10906 1998 5929 1012 1996 1000 2067 2000 1996 3011 1000 4695 1998 1996 3037 1999 4424 8993 1998 5850 2008 2001 2025 12099 5476 3372 1010 2004 6791 1999 2116 2101 4617 1997 1996 3963 1005 1055 2001 2157 2006 1010 2004 2001 1996 1000 2123 1005 1056 3198 2123 1005 1056 2425 1000 18404 1011 26505 1997 1996 13577 2081 2039 1997 2264 1011 3023 17412 1010 3272 2008 1045 2052 2031 2245 2023 2001 2062 3497 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.329466 140109432092480 run_classifier.py:465] input_ids: 101 1045 10468 2179 10267 1005 1055 7774 2000 2022 1037 2200 9996 3833 2008 2081 2009 3697 2000 3422 1012 2174 1010 2045 2003 2242 1045 2442 2360 2055 2129 1996 2472 4110 2242 2055 1996 7224 1997 1996 2220 3963 1005 1055 1999 1996 3601 1997 10906 1998 5929 1012 1996 1000 2067 2000 1996 3011 1000 4695 1998 1996 3037 1999 4424 8993 1998 5850 2008 2001 2025 12099 5476 3372 1010 2004 6791 1999 2116 2101 4617 1997 1996 3963 1005 1055 2001 2157 2006 1010 2004 2001 1996 1000 2123 1005 1056 3198 2123 1005 1056 2425 1000 18404 1011 26505 1997 1996 13577 2081 2039 1997 2264 1011 3023 17412 1010 3272 2008 1045 2052 2031 2245 2023 2001 2062 3497 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.330152 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.331130 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.331893 140109432092480 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.338274 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.344489 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] fi ##do is a story about more well manner ##ed zombies who have been trained to serve the human race . all falls apart though , when young timmy ' s zombie fi ##do eats the family neighbor . from then on , disaster ( well , maybe not disaster , but to some extent , chaos ) occurs . most of the people treat their zombies with fairness , and one such character sleeps with his zombie ( very funny part of the movie , if not also very disturbing too ) . and we find the loving fi ##do whatever he may do . this is a very funny and unique film , especially for the zombie genre . it is also probably one [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.346609 140109432092480 run_classifier.py:464] tokens: [CLS] fi ##do is a story about more well manner ##ed zombies who have been trained to serve the human race . all falls apart though , when young timmy ' s zombie fi ##do eats the family neighbor . from then on , disaster ( well , maybe not disaster , but to some extent , chaos ) occurs . most of the people treat their zombies with fairness , and one such character sleeps with his zombie ( very funny part of the movie , if not also very disturbing too ) . and we find the loving fi ##do whatever he may do . this is a very funny and unique film , especially for the zombie genre . it is also probably one [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10882 3527 2003 1037 2466 2055 2062 2092 5450 2098 14106 2040 2031 2042 4738 2000 3710 1996 2529 2679 1012 2035 4212 4237 2295 1010 2043 2402 27217 1005 1055 11798 10882 3527 20323 1996 2155 11429 1012 2013 2059 2006 1010 7071 1006 2092 1010 2672 2025 7071 1010 2021 2000 2070 6698 1010 8488 1007 5158 1012 2087 1997 1996 2111 7438 2037 14106 2007 26935 1010 1998 2028 2107 2839 25126 2007 2010 11798 1006 2200 6057 2112 1997 1996 3185 1010 2065 2025 2036 2200 14888 2205 1007 1012 1998 2057 2424 1996 8295 10882 3527 3649 2002 2089 2079 1012 2023 2003 1037 2200 6057 1998 4310 2143 1010 2926 2005 1996 11798 6907 1012 2009 2003 2036 2763 2028 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.347511 140109432092480 run_classifier.py:465] input_ids: 101 10882 3527 2003 1037 2466 2055 2062 2092 5450 2098 14106 2040 2031 2042 4738 2000 3710 1996 2529 2679 1012 2035 4212 4237 2295 1010 2043 2402 27217 1005 1055 11798 10882 3527 20323 1996 2155 11429 1012 2013 2059 2006 1010 7071 1006 2092 1010 2672 2025 7071 1010 2021 2000 2070 6698 1010 8488 1007 5158 1012 2087 1997 1996 2111 7438 2037 14106 2007 26935 1010 1998 2028 2107 2839 25126 2007 2010 11798 1006 2200 6057 2112 1997 1996 3185 1010 2065 2025 2036 2200 14888 2205 1007 1012 1998 2057 2424 1996 8295 10882 3527 3649 2002 2089 2079 1012 2023 2003 1037 2200 6057 1998 4310 2143 1010 2926 2005 1996 11798 6907 1012 2009 2003 2036 2763 2028 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.348276 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.349129 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:15.349792 140109432092480 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.545776 140109432092480 run_classifier.py:774] Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.549091 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.550209 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i thought this was an extremely bad movie . the whole time i was watching this movie i couldn ' t help but think over and over how bad it is , and how that was $ 3 . 69 down the drain . the plot was so jump ##y . they did an excellent job at the beginning of explaining who dated who in high school , but they never really explained anything after that . was it a supernatural thriller ? was it a regular thriller ? apparently you can decide for yourself , because they didn ' t see the need to explain . i understood basically what happened , i think . what i got confused about was all of it prior [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.551109 140109432092480 run_classifier.py:464] tokens: [CLS] i thought this was an extremely bad movie . the whole time i was watching this movie i couldn ' t help but think over and over how bad it is , and how that was $ 3 . 69 down the drain . the plot was so jump ##y . they did an excellent job at the beginning of explaining who dated who in high school , but they never really explained anything after that . was it a supernatural thriller ? was it a regular thriller ? apparently you can decide for yourself , because they didn ' t see the need to explain . i understood basically what happened , i think . what i got confused about was all of it prior [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2245 2023 2001 2019 5186 2919 3185 1012 1996 2878 2051 1045 2001 3666 2023 3185 1045 2481 1005 1056 2393 2021 2228 2058 1998 2058 2129 2919 2009 2003 1010 1998 2129 2008 2001 1002 1017 1012 6353 2091 1996 12475 1012 1996 5436 2001 2061 5376 2100 1012 2027 2106 2019 6581 3105 2012 1996 2927 1997 9990 2040 6052 2040 1999 2152 2082 1010 2021 2027 2196 2428 4541 2505 2044 2008 1012 2001 2009 1037 11189 10874 1029 2001 2009 1037 3180 10874 1029 4593 2017 2064 5630 2005 4426 1010 2138 2027 2134 1005 1056 2156 1996 2342 2000 4863 1012 1045 5319 10468 2054 3047 1010 1045 2228 1012 2054 1045 2288 5457 2055 2001 2035 1997 2009 3188 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.551878 140109432092480 run_classifier.py:465] input_ids: 101 1045 2245 2023 2001 2019 5186 2919 3185 1012 1996 2878 2051 1045 2001 3666 2023 3185 1045 2481 1005 1056 2393 2021 2228 2058 1998 2058 2129 2919 2009 2003 1010 1998 2129 2008 2001 1002 1017 1012 6353 2091 1996 12475 1012 1996 5436 2001 2061 5376 2100 1012 2027 2106 2019 6581 3105 2012 1996 2927 1997 9990 2040 6052 2040 1999 2152 2082 1010 2021 2027 2196 2428 4541 2505 2044 2008 1012 2001 2009 1037 11189 10874 1029 2001 2009 1037 3180 10874 1029 4593 2017 2064 5630 2005 4426 1010 2138 2027 2134 1005 1056 2156 1996 2342 2000 4863 1012 1045 5319 10468 2054 3047 1010 1045 2228 1012 2054 1045 2288 5457 2055 2001 2035 1997 2009 3188 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.552636 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.554535 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.556530 140109432092480 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.559410 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.560201 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i was told jon was for awhile on spiritual experiences . i guessed the film will be interesting . in fact isn ' t at all . not so much profound for a such subject . \" eternity \" never - ending life . experiences after death and \" de ##ja ##vu \" . the film is not as a comedy but isn ' t funny at all , at least not express yet . it ' s so naive . charming film but naive film . a must to avoid . the middle ages sequences seems coming directly from fairy tales and it ' s not the matter at all . eileen davidson is so charming and vo ##ight is doing his best . normal [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.560904 140109432092480 run_classifier.py:464] tokens: [CLS] i was told jon was for awhile on spiritual experiences . i guessed the film will be interesting . in fact isn ' t at all . not so much profound for a such subject . \" eternity \" never - ending life . experiences after death and \" de ##ja ##vu \" . the film is not as a comedy but isn ' t funny at all , at least not express yet . it ' s so naive . charming film but naive film . a must to avoid . the middle ages sequences seems coming directly from fairy tales and it ' s not the matter at all . eileen davidson is so charming and vo ##ight is doing his best . normal [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2001 2409 6285 2001 2005 19511 2006 6259 6322 1012 1045 11445 1996 2143 2097 2022 5875 1012 1999 2755 3475 1005 1056 2012 2035 1012 2025 2061 2172 13769 2005 1037 2107 3395 1012 1000 12715 1000 2196 1011 4566 2166 1012 6322 2044 2331 1998 1000 2139 3900 19722 1000 1012 1996 2143 2003 2025 2004 1037 4038 2021 3475 1005 1056 6057 2012 2035 1010 2012 2560 2025 4671 2664 1012 2009 1005 1055 2061 15743 1012 11951 2143 2021 15743 2143 1012 1037 2442 2000 4468 1012 1996 2690 5535 10071 3849 2746 3495 2013 8867 7122 1998 2009 1005 1055 2025 1996 3043 2012 2035 1012 20495 12017 2003 2061 11951 1998 29536 18743 2003 2725 2010 2190 1012 3671 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.561580 140109432092480 run_classifier.py:465] input_ids: 101 1045 2001 2409 6285 2001 2005 19511 2006 6259 6322 1012 1045 11445 1996 2143 2097 2022 5875 1012 1999 2755 3475 1005 1056 2012 2035 1012 2025 2061 2172 13769 2005 1037 2107 3395 1012 1000 12715 1000 2196 1011 4566 2166 1012 6322 2044 2331 1998 1000 2139 3900 19722 1000 1012 1996 2143 2003 2025 2004 1037 4038 2021 3475 1005 1056 6057 2012 2035 1010 2012 2560 2025 4671 2664 1012 2009 1005 1055 2061 15743 1012 11951 2143 2021 15743 2143 1012 1037 2442 2000 4468 1012 1996 2690 5535 10071 3849 2746 3495 2013 8867 7122 1998 2009 1005 1055 2025 1996 3043 2012 2035 1012 20495 12017 2003 2061 11951 1998 29536 18743 2003 2725 2010 2190 1012 3671 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.562242 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.563027 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.563889 140109432092480 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.567308 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.568290 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this show can most accurately be described as tv bubble - gum : it ' s chewing , it has a good taste to it , and it lasts a long time . but , like bubble - gum , it can leave a bad taste in your mouth after a few too many chew ##s . < br / > < br / > this show features very simple questions that take the form of simply games , like hang ##man , guessing a simple phrase or guessing items on a short list . caller ##s are the contestants , and anybody can play . < br / > < br / > the questions , though , are terribly easy . there ' s [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.569087 140109432092480 run_classifier.py:464] tokens: [CLS] this show can most accurately be described as tv bubble - gum : it ' s chewing , it has a good taste to it , and it lasts a long time . but , like bubble - gum , it can leave a bad taste in your mouth after a few too many chew ##s . < br / > < br / > this show features very simple questions that take the form of simply games , like hang ##man , guessing a simple phrase or guessing items on a short list . caller ##s are the contestants , and anybody can play . < br / > < br / > the questions , though , are terribly easy . there ' s [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2265 2064 2087 14125 2022 2649 2004 2694 11957 1011 16031 1024 2009 1005 1055 17492 1010 2009 2038 1037 2204 5510 2000 2009 1010 1998 2009 16180 1037 2146 2051 1012 2021 1010 2066 11957 1011 16031 1010 2009 2064 2681 1037 2919 5510 1999 2115 2677 2044 1037 2261 2205 2116 21271 2015 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2265 2838 2200 3722 3980 2008 2202 1996 2433 1997 3432 2399 1010 2066 6865 2386 1010 16986 1037 3722 7655 2030 16986 5167 2006 1037 2460 2862 1012 20587 2015 2024 1996 10584 1010 1998 10334 2064 2377 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 3980 1010 2295 1010 2024 16668 3733 1012 2045 1005 1055 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.569864 140109432092480 run_classifier.py:465] input_ids: 101 2023 2265 2064 2087 14125 2022 2649 2004 2694 11957 1011 16031 1024 2009 1005 1055 17492 1010 2009 2038 1037 2204 5510 2000 2009 1010 1998 2009 16180 1037 2146 2051 1012 2021 1010 2066 11957 1011 16031 1010 2009 2064 2681 1037 2919 5510 1999 2115 2677 2044 1037 2261 2205 2116 21271 2015 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2265 2838 2200 3722 3980 2008 2202 1996 2433 1997 3432 2399 1010 2066 6865 2386 1010 16986 1037 3722 7655 2030 16986 5167 2006 1037 2460 2862 1012 20587 2015 2024 1996 10584 1010 1998 10334 2064 2377 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 3980 1010 2295 1010 2024 16668 3733 1012 2045 1005 1055 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.570600 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.571281 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.571841 140109432092480 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.576718 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.577685 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i un ##wi ##tting ##ly walked into this \" trap \" of a movie . < br / > < br / > if i could turn back time or simply get a ref ##und i would be happy . < br / > < br / > it was 7 : 30 ##pm and cinderella man didn ' t start until 10 ##pm so i rushed into the theater to catch the movie that started at 7 : 20 ##pm . . . and i dare say god reached out his hand ( or retracted it ) and allowed me to punish myself for my film g ##lu ##tton ##y by sitting in for this film . < br / > < br / > [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.578574 140109432092480 run_classifier.py:464] tokens: [CLS] i un ##wi ##tting ##ly walked into this \" trap \" of a movie . < br / > < br / > if i could turn back time or simply get a ref ##und i would be happy . < br / > < br / > it was 7 : 30 ##pm and cinderella man didn ' t start until 10 ##pm so i rushed into the theater to catch the movie that started at 7 : 20 ##pm . . . and i dare say god reached out his hand ( or retracted it ) and allowed me to punish myself for my film g ##lu ##tton ##y by sitting in for this film . < br / > < br / > [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 4895 9148 13027 2135 2939 2046 2023 1000 8132 1000 1997 1037 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2065 1045 2071 2735 2067 2051 2030 3432 2131 1037 25416 8630 1045 2052 2022 3407 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2001 1021 1024 2382 9737 1998 21686 2158 2134 1005 1056 2707 2127 2184 9737 2061 1045 6760 2046 1996 4258 2000 4608 1996 3185 2008 2318 2012 1021 1024 2322 9737 1012 1012 1012 1998 1045 8108 2360 2643 2584 2041 2010 2192 1006 2030 28214 2009 1007 1998 3039 2033 2000 16385 2870 2005 2026 2143 1043 7630 15474 2100 2011 3564 1999 2005 2023 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.579349 140109432092480 run_classifier.py:465] input_ids: 101 1045 4895 9148 13027 2135 2939 2046 2023 1000 8132 1000 1997 1037 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2065 1045 2071 2735 2067 2051 2030 3432 2131 1037 25416 8630 1045 2052 2022 3407 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2001 1021 1024 2382 9737 1998 21686 2158 2134 1005 1056 2707 2127 2184 9737 2061 1045 6760 2046 1996 4258 2000 4608 1996 3185 2008 2318 2012 1021 1024 2322 9737 1012 1012 1012 1998 1045 8108 2360 2643 2584 2041 2010 2192 1006 2030 28214 2009 1007 1998 3039 2033 2000 16385 2870 2005 2026 2143 1043 7630 15474 2100 2011 3564 1999 2005 2023 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.580029 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.580804 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.581396 140109432092480 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.584621 140109432092480 run_classifier.py:461] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.585539 140109432092480 run_classifier.py:462] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] there are two ways to turn a tv series into a film . < br / > < br / > the first , most common , and least successful , is to basically make a feature - length tv episode - see the disasters of the step ##to ##e & son movie . the second is to do something else - something quite different , a la monty python . < br / > < br / > thankfully , the creators of the cult tv series have gone for the second option , and they ' ve come up with something unique , clever and funny - it couldn ' t feel less like a tv episode . < br / > < br [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.586246 140109432092480 run_classifier.py:464] tokens: [CLS] there are two ways to turn a tv series into a film . < br / > < br / > the first , most common , and least successful , is to basically make a feature - length tv episode - see the disasters of the step ##to ##e & son movie . the second is to do something else - something quite different , a la monty python . < br / > < br / > thankfully , the creators of the cult tv series have gone for the second option , and they ' ve come up with something unique , clever and funny - it couldn ' t feel less like a tv episode . < br / > < br [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2045 2024 2048 3971 2000 2735 1037 2694 2186 2046 1037 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2034 1010 2087 2691 1010 1998 2560 3144 1010 2003 2000 10468 2191 1037 3444 1011 3091 2694 2792 1011 2156 1996 18665 1997 1996 3357 3406 2063 1004 2365 3185 1012 1996 2117 2003 2000 2079 2242 2842 1011 2242 3243 2367 1010 1037 2474 18446 18750 1012 1026 7987 1013 1028 1026 7987 1013 1028 16047 1010 1996 17277 1997 1996 8754 2694 2186 2031 2908 2005 1996 2117 5724 1010 1998 2027 1005 2310 2272 2039 2007 2242 4310 1010 12266 1998 6057 1011 2009 2481 1005 1056 2514 2625 2066 1037 2694 2792 1012 1026 7987 1013 1028 1026 7987 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.586902 140109432092480 run_classifier.py:465] input_ids: 101 2045 2024 2048 3971 2000 2735 1037 2694 2186 2046 1037 2143 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2034 1010 2087 2691 1010 1998 2560 3144 1010 2003 2000 10468 2191 1037 3444 1011 3091 2694 2792 1011 2156 1996 18665 1997 1996 3357 3406 2063 1004 2365 3185 1012 1996 2117 2003 2000 2079 2242 2842 1011 2242 3243 2367 1010 1037 2474 18446 18750 1012 1026 7987 1013 1028 1026 7987 1013 1028 16047 1010 1996 17277 1997 1996 8754 2694 2186 2031 2908 2005 1996 2117 5724 1010 1998 2027 1005 2310 2272 2039 2007 2242 4310 1010 12266 1998 6057 1011 2009 2481 1005 1056 2514 2625 2066 1037 2694 2792 1012 1026 7987 1013 1028 1026 7987 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.587491 140109432092480 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.589235 140109432092480 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:31.590011 140109432092480 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'tf_out', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6d539d1b70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:47.189294 140109432092480 estimator.py:201] Using config: {'_model_dir': 'tf_out', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6d539d1b70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:48.989290 140109432092480 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:46:51.442659 140109432092480 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0503 16:46:51.555740 140109432092480 deprecation.py:506] From <ipython-input-13-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0503 16:46:51.597835 140109432092480 deprecation.py:323] From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0503 16:46:51.663271 140109432092480 deprecation.py:323] From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0503 16:46:58.140927 140109432092480 deprecation.py:323] From /home/ArjunShukla/.local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:12.823687 140109432092480 estimator.py:1113] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:12.826214 140109432092480 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:15.892471 140109432092480 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:20.631541 140109432092480 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:20.854492 140109432092480 session_manager.py:493] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into tf_out/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:47:29.267897 140109432092480 basic_session_run_hooks.py:594] Saving checkpoints for 0 into tf_out/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6883311, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 16:52:25.956855 140109432092480 basic_session_run_hooks.py:249] loss = 0.6883311, step = 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")\n",
    "\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "OUTPUT_DIR = 'tf_out'\n",
    "\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n",
    "\n",
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)\n",
    "\n",
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
    "\n",
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fd2951b82103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
