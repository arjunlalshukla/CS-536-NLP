{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import *\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pymagnitude import *\n",
    "import time\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/questions.csv')\n",
    "data = data\n",
    "path = 'data/GoogleNews-vectors-negative300.magnitude'\n",
    "vectors = Magnitude(path)\n",
    "tok = RegexpTokenizer('\\w+')\n",
    "data.question1 = pd.Series([tok.tokenize(str(s)) for s in data.question1])\n",
    "data.question2 = pd.Series([tok.tokenize(str(s)) for s in data.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions and constants\n",
    "def sum_vec(words):\n",
    "    res = np.sum([vectors.query(w) for w in words if w in vectors], axis=0)\n",
    "    if type(res) is np.float64:\n",
    "        return np.array([1e-32] * vectors.dim)\n",
    "    else:\n",
    "        return res\n",
    "    \n",
    "def rm_stop(words):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in words if not w in stop_words]\n",
    "\n",
    "def train_test(col1 ,col2):\n",
    "    sep = int(len(data)*.75)\n",
    "    ret = {}\n",
    "    ret['train_pnts'] = data.as_matrix([col1, col2])[:sep]\n",
    "    ret['train_pnts'] = [[cosine(x[0], x[1])] for x in ret['train_pnts']]\n",
    "    ret['train_tgts'] = data.as_matrix([\"is_duplicate\"])[:sep]\n",
    "    ret['test_pnts']  = data.as_matrix([col1, col2])[sep:]\n",
    "    ret['test_pnts']  = [[cosine(x[0], x[1])] for x in ret['test_pnts']]\n",
    "    ret['test_tgts']  = data.as_matrix([\"is_duplicate\"])[sep:]\n",
    "    return ret\n",
    "\n",
    "i_tup_num = 0\n",
    "i_question1 = 3\n",
    "i_question2 = 4\n",
    "i_is_duplicate = 5\n",
    "i_vec1 = 6\n",
    "i_vec2 = 7\n",
    "i_stop1 = 8\n",
    "i_stop_vec1 = 9\n",
    "i_stop2 = 10\n",
    "i_stop_vec2 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "vec1 = pd.Series(sum_vec(words) for words in data.question1)\n",
    "data.insert(len(data.columns), \"vec1\", vec1)\n",
    "print(\"Finished vec1\")\n",
    "vec2 = pd.Series(sum_vec(words) for words in data.question2)\n",
    "data.insert(len(data.columns), \"vec2\", vec2)\n",
    "print(\"Finished vec2\")\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "stop1 = pd.Series(rm_stop(words) for words in data.question1)\n",
    "data.insert(len(data.columns), \"stop1\", stop1)\n",
    "stop_vec1 = pd.Series(sum_vec(words) for words in data.stop1)\n",
    "data.insert(len(data.columns), \"stop_vec1\", stop_vec1)\n",
    "print(\"Finished stop1\")\n",
    "stop2 = pd.Series(rm_stop(words) for words in data.question2)\n",
    "data.insert(len(data.columns), \"stop2\", stop2)\n",
    "stop_vec2 = pd.Series(sum_vec(words) for words in data.stop2)\n",
    "data.insert(len(data.columns), \"stop_vec2\", stop_vec1)\n",
    "print(\"Finished stop2\")\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial data information\n",
    "print(\"# of data points:\", len(data))\n",
    "print(\"# of word vectors:\", len(vectors))\n",
    "print('vector dimensions:', vectors.dim)\n",
    "cnt = Counter(data.is_duplicate)\n",
    "print(\"random baseline:\", 1/len(cnt.keys()))\n",
    "print(\"most common baseline:\", max([cnt[k]/sum(cnt.values()) for k in cnt.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(\n",
    "    [0 if nltk.translate.bleu_score.sentence_bleu(\n",
    "    [data.question1[i]], \n",
    "    data.question2[i]) < .5 else 1\n",
    "    for i in range(len(data))],\n",
    "    data.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_test(\"vec1\", \"vec2\")\n",
    "lr = LogisticRegression().fit(x['train_pnts'], x['train_tgts'])\n",
    "print(\"normal embeddings LR accuracy:\", accuracy_score(lr.predict(x['test_pnts']), x['test_tgts']))\n",
    "print(\"normal embeddings LR coefficients:\", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_test(\"stop_vec1\", \"stop_vec2\")\n",
    "lr = LogisticRegression().fit(x['train_pnts'], x['train_tgts'])\n",
    "print(\"no stop words LR accuracy:\", accuracy_score(lr.predict(x['test_pnts']), x['test_tgts']))\n",
    "print(\"no stop words LR coefficients:\", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x in data.values:\n",
    "    if x[i_question1] == x[i_question2]:\n",
    "        print(x[i_is_duplicate], x[i_tup_num])\n",
    "        print(x[i_question1])\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = data.as_matrix([\"vec1\", \"vec2\"])\n",
    "hello = [[cosine(x[0], x[1])] for x in hello]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.values\n",
    "k = 0\n",
    "for i in range(len(data.values)):\n",
    "    if hello[i] == [0.0]:\n",
    "        print(x[i][i_is_duplicate], x[i][i_tup_num])\n",
    "        print(x[i][i_question1])\n",
    "        print(x[i][i_question2])\n",
    "        k += 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vectors.query(\"PhD\"), vectors.query(\"Ph.D.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"WW1\" in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Chick-fil-A\" in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[299692:299692+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "trees = treebank.parsed_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'dsafa'\n",
    "x.hello = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(tree):\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return child_nodes\n",
    "\n",
    "trees = [subtree for tree in treebank.parsed_sents() for subtree in tree.subtrees(filter)]\n",
    "\n",
    "production_rules = defaultdict(list)\n",
    "terminals = list()\n",
    "\n",
    "for tree in trees:\n",
    "    for prod in tree.productions():\n",
    "        lhs = str(prod.lhs()).replace('=','-').replace('|','-')\n",
    "        if prod.is_lexical():\n",
    "            terminals.append(lhs)\n",
    "        if prod.is_nonlexical():\n",
    "            rhs = ''\n",
    "            for s in prod.rhs(): \n",
    "                rhs += str(s).replace('=','-').replace('|','-') + ' '\n",
    "            production_rules[lhs].append(str(rhs.strip()))\n",
    "            \n",
    "terminals = set(terminals)\n",
    "terminals.remove(\"''\")\n",
    "\n",
    "production_counters = dict()\n",
    "\n",
    "for prod in production_rules:\n",
    "    production_counters[prod] = Counter(production_rules[prod])\n",
    "\n",
    "def to_prod_rules(lhs, c):\n",
    "    global terminals\n",
    "    denom = sum(c.values())\n",
    "    cfg_str = list()\n",
    "    for rhs in c:\n",
    "        rhs_prods = rhs.split()\n",
    "        r = ''\n",
    "        for rule in rhs_prods: \n",
    "            if rule == '' or rule == \"''\": continue\n",
    "            r += \"'\" + rule + \"' \" if rule in terminals else rule + \" \"\n",
    "        cfg_str.append(\"{} -> {} [{}]\".format(lhs, r.strip(), '{:.20f}'.format(c[rhs] / denom)))\n",
    "    return cfg_str\n",
    "\n",
    "production_lhs = list(production_counters.keys())\n",
    "production_lhs.remove('S')\n",
    "production_lhs = ['S'] + production_lhs\n",
    "\n",
    "prod_rules = list()\n",
    "\n",
    "for lhs in production_lhs:\n",
    "    prod_rules += to_prod_rules(lhs, production_counters[lhs])\n",
    "    \n",
    "prod_rules_str = '\\n'.join(prod_rules)\n",
    "\n",
    "grammar = nltk.PCFG.fromstring(prod_rules_str)\n",
    "\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['I', 'like', 'to', 'eat', 'tacos']\n",
    "\n",
    "pos_sentence = nltk.pos_tag(sentence)\n",
    "\n",
    "print(sentence)\n",
    "print(pos_sentence)\n",
    "\n",
    "\n",
    "for tree in viterbi_parser.parse([x[1] for x in pos_sentence]):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['i'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
